{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname\n",
    "\n",
    "# Model Loading\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "import transformers\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "path=\"./llama_hf/13b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(path, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path, device_map=\"cuda\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"Total Memory: {gpu_props.total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"Memory Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts\n",
    "\n",
    "# from tools import task_generation\n",
    "# importlib.reload(task_generation)\n",
    "\n",
    "from tools.task_generation import gen_task\n",
    "\n",
    "batch_size = 200\n",
    "task=gen_task(prompt_num=batch_size, example_num=5, same_task=False, seed=0)\n",
    "task_target=gen_task(prompt_num=batch_size, example_num=5, same_task=False, seed=1)\n",
    "print(task_target.answers)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def reshape(response:str):\n",
    "    response_re=deepcopy(response)\n",
    "    while response_re!=\"\" and (response_re[0]==' ' or response_re[0] == '\\n'):\n",
    "        response_re=response_re[1:]\n",
    "    response_re=response_re.split('\\n')[0]\n",
    "    response_re=response_re.split(' ')[0]\n",
    "    return response_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and calculate accuracy\n",
    "\n",
    "import numpy as np\n",
    "from tools.inference import forward, modified_forward\n",
    "from tools.utils import show_outputs, extract_hidden\n",
    "\n",
    "def match_count(responses:list, answers:list, to_chr=-1):\n",
    "    responses_mod=[None for _ in range(len(responses))]\n",
    "    answers_mod=[None for _ in range(len(answers))]\n",
    "    \n",
    "    for index in range(len(responses)):\n",
    "        res_len=len(reshape(responses[index]))\n",
    "        ans_len=len(reshape(answers[index]))\n",
    "        if to_chr>0:\n",
    "            mod_len=min(to_chr,res_len)\n",
    "        else:\n",
    "            mod_len=res_len\n",
    "\n",
    "        if mod_len>0:\n",
    "            responses_mod[index]=reshape(responses[index])[:min(mod_len,ans_len)]\n",
    "            answers_mod[index]=reshape(answers[index])[:min(mod_len,ans_len)]\n",
    "        else:\n",
    "            responses_mod[index]=\"\"\n",
    "            answers_mod[index]=answers[index]\n",
    "\n",
    "    count=(np.array(responses_mod)==np.array(answers_mod)).sum()\n",
    "    total_num=len(responses)\n",
    "    return count/total_num\n",
    "\n",
    "record=[]\n",
    "for lg_id in [0,1,2]:\n",
    "    print(f\"Language id: {lg_id}\")\n",
    "    prompts=task.few_shot[lg_id]\n",
    "    outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "\n",
    "    prompts_icl = task_target.few_shot[lg_id]\n",
    "    outputs_icl=forward(model, tokenizer, prompts_icl, gen_len=4)\n",
    "    responses_icl=show_outputs(tokenizer, outputs_icl, without_prompt=prompts_icl)\n",
    "\n",
    "    prompts_raw = task_target.zero_shot[lg_id]\n",
    "    outputs_raw=forward(model, tokenizer, prompts_raw, gen_len=4)\n",
    "    responses_raw=show_outputs(tokenizer, outputs_raw, without_prompt=prompts_raw)\n",
    "\n",
    "    answers = task_target.answers\n",
    "\n",
    "    acc_raw=match_count(responses_raw, answers)\n",
    "    acc_icl=match_count(responses_icl, answers)\n",
    "    print(f\"Original  {acc_raw:.2f}\")\n",
    "    print(f\"ICL       {acc_icl:.2f}\")\n",
    "\n",
    "    t_v_acc_list=[]\n",
    "    for target_layer_id in range(10,16):\n",
    "        hiddens_tv=extract_hidden(outputs, layer_id=target_layer_id)\n",
    "        outputs_raw_tv=modified_forward(model, tokenizer, prompts_raw, target_layer_id, hiddens_tv, gen_len=4)\n",
    "        responses_raw_tv=show_outputs(tokenizer, outputs_raw_tv, without_prompt=prompts_raw)\n",
    "        t_v_acc=match_count(responses_raw_tv, answers)\n",
    "        t_v_acc_list.append(t_v_acc)\n",
    "        print(f\"Patching layer {target_layer_id:02d}  {t_v_acc:.2f}\")\n",
    "\n",
    "    t_v_acc_max=max(t_v_acc_list)\n",
    "    record.append([acc_raw, acc_icl, t_v_acc_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = ['fr2en', 'sp2en', 'it2en']\n",
    "conditions = ['few shot', 'task vector', 'zero shot']\n",
    "accuracy = np.array(record)\n",
    "\n",
    "df = pd.DataFrame(accuracy, index=models, columns=conditions).reset_index().melt(id_vars='index')\n",
    "df.columns = ['Model', 'Condition', 'Accuracy']\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"Accuracy\", y=\"Model\", hue=\"Condition\",\n",
    "    errorbar='sd', palette=\"bright\",\n",
    "    alpha=0.8, height=6, aspect=1, width=0.5,\n",
    ")\n",
    "\n",
    "# Adjust the layout\n",
    "g.set_xticklabels([0,0.2,0.4,0.6,0.8,1])\n",
    "# g.despine(left=True)\n",
    "g.set_axis_labels(\"Accuracy\", \"Task\")\n",
    "g.legend.set_title(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.task_generation import gen_task\n",
    "\n",
    "batch_size = 200\n",
    "task_train=gen_task(prompt_num=batch_size, example_num=5, same_task=False, seed=2)\n",
    "task=gen_task(prompt_num=batch_size, example_num=5, same_task=False, seed=0)\n",
    "task_target=gen_task(prompt_num=batch_size, example_num=5, same_task=False, seed=1)\n",
    "print(task_target.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: proof of concept\n",
    "\n",
    "import numpy as np\n",
    "from tools.inference import forward, modified_forward\n",
    "from tools.utils import show_outputs, extract_hidden\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "source_layer_id=39\n",
    "target_layer_id=13\n",
    "\n",
    "hiddens_list=[]\n",
    "hiddens_tv_list=[]\n",
    "for lg_id in [0,1,2]:\n",
    "    prompts=task_train.few_shot[lg_id]\n",
    "    outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "    hiddens=extract_hidden(outputs, layer_id=source_layer_id)\n",
    "    hiddens_list.append(hiddens)\n",
    "    hiddens_tv=extract_hidden(outputs, layer_id=target_layer_id)\n",
    "    hiddens_tv_list.append(hiddens_tv)\n",
    "\n",
    "DATA_train=torch.cat(hiddens_list,dim=0)\n",
    "data_train=DATA_train.cpu().numpy()\n",
    "\n",
    "# Training\n",
    "classifier = LogisticRegression(max_iter=200)\n",
    "classifier.fit(data_train, labels)\n",
    "\n",
    "# Testing\n",
    "hiddens_list=[]\n",
    "hiddens_tv_list=[]\n",
    "for lg_id in [0,1,2]:\n",
    "    prompts=task.few_shot[lg_id]\n",
    "    outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "    hiddens=extract_hidden(outputs, layer_id=source_layer_id)\n",
    "    hiddens_list.append(hiddens)\n",
    "    hiddens_tv=extract_hidden(outputs, layer_id=target_layer_id)\n",
    "    hiddens_tv_list.append(hiddens_tv)\n",
    "\n",
    "DATA_test=torch.cat(hiddens_list,dim=0)\n",
    "data_test=DATA_test.cpu().numpy()\n",
    "\n",
    "y_pred = classifier.predict(data_test)\n",
    "accuracy = accuracy_score(labels, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and extract hidden states\n",
    "\n",
    "import numpy as np\n",
    "from tools.inference import forward, modified_forward\n",
    "from tools.utils import show_outputs, extract_hidden\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def match_count(responses:list, answers:list, to_chr=-1):\n",
    "    responses_mod=[None for _ in range(len(responses))]\n",
    "    answers_mod=[None for _ in range(len(answers))]\n",
    "    \n",
    "    for index in range(len(responses)):\n",
    "        res_len=len(reshape(responses[index]))\n",
    "        ans_len=len(reshape(answers[index]))\n",
    "        if to_chr>0:\n",
    "            mod_len=min(to_chr,res_len)\n",
    "        else:\n",
    "            mod_len=res_len\n",
    "\n",
    "        if mod_len>0:\n",
    "            responses_mod[index]=reshape(responses[index])[:min(mod_len,ans_len)]\n",
    "            answers_mod[index]=reshape(answers[index])[:min(mod_len,ans_len)]\n",
    "        else:\n",
    "            responses_mod[index]=\"\"\n",
    "            answers_mod[index]=answers[index]\n",
    "\n",
    "    # print(responses_mod[0:10])\n",
    "    # print(answers_mod[0:10])\n",
    "\n",
    "    count=(np.array(responses_mod)==np.array(answers_mod)).sum()\n",
    "    total_num=len(responses)\n",
    "    return count/total_num\n",
    "\n",
    "source_layer_id=39\n",
    "target_layer_id=13\n",
    "\n",
    "record2=[]\n",
    "layer_list=[39]\n",
    "\n",
    "for source_layer_id in layer_list:\n",
    "    print(f\"Recover: {source_layer_id} -> {target_layer_id}\")\n",
    "\n",
    "    hiddens_list=[]\n",
    "    hiddens_tv_list=[]\n",
    "    for lg_id in [0,1,2]:\n",
    "        prompts=task_train.few_shot[lg_id]\n",
    "        outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "        hiddens=extract_hidden(outputs, layer_id=source_layer_id)\n",
    "        hiddens_list.append(hiddens)\n",
    "        hiddens_tv=extract_hidden(outputs, layer_id=target_layer_id)\n",
    "        hiddens_tv_list.append(hiddens_tv)\n",
    "\n",
    "    DATA_last_train=torch.cat(hiddens_list,dim=0)\n",
    "    DATA_tv_train=torch.cat(hiddens_tv_list,dim=0)\n",
    "    X_train=DATA_last_train.cpu().numpy()\n",
    "    Y_train=DATA_tv_train.cpu().numpy()\n",
    "\n",
    "    ln_reg = LinearRegression()\n",
    "    ln_reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lg_id in [0,1,2]:\n",
    "    prompts=task.few_shot[lg_id]\n",
    "    outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "    hiddens=extract_hidden(outputs, layer_id=source_layer_id)\n",
    "\n",
    "    tv_pred = ln_reg.predict(hiddens.cpu().numpy())\n",
    "    hiddens_tv_pred=torch.tensor(tv_pred, dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "    prompts_test=task_target.zero_shot[lg_id]\n",
    "    answers_test=task_target.answers\n",
    "\n",
    "    outputs_raw_tv=modified_forward(model, tokenizer, prompts_test, target_layer_id, hiddens_tv_pred, gen_len=3)\n",
    "    responses_raw_tv=show_outputs(tokenizer, outputs_raw_tv, without_prompt=prompts_test)\n",
    "    acc=match_count(responses_raw_tv, answers_test)\n",
    "    print(f\"Languge id:{lg_id}  Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_add=[[0.05, 0.84, 0.675, 0.64], [0.05, 0.83, 0.705, 0.69], [0.05, 0.885, 0.765, 0.75]]\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = ['fr2en', 'sp2en', 'it2en']\n",
    "conditions = ['zero shot', 'few shot', 'task vector', 'recovered\\ntask vector']\n",
    "accuracy = np.array(record_add)\n",
    "\n",
    "df = pd.DataFrame(accuracy, index=models, columns=conditions).reset_index().melt(id_vars='index')\n",
    "df.columns = ['Model', 'Condition', 'Accuracy']\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"Accuracy\", y=\"Model\", hue=\"Condition\",\n",
    "    errorbar='sd', palette=\"bright\", alpha=0.8, height=6, aspect=1, width=0.5\n",
    ")\n",
    "\n",
    "# Adjust the layout\n",
    "g.set_xticklabels([0,0.2,0.4,0.6,0.8,1])\n",
    "# g.despine(left=True)\n",
    "g.set_axis_labels(\"Accuracy\", \"Task\")\n",
    "g.legend.set_title(\"\")\n",
    "\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts\n",
    "\n",
    "# from tools import task_generation\n",
    "# importlib.reload(task_generation)\n",
    "\n",
    "from tools.task_generation import gen_task\n",
    "\n",
    "batch_size = 200\n",
    "task=gen_task(prompt_num=batch_size, example_num=5, same_task=True, seed=0)\n",
    "\n",
    "source_layer_id=39\n",
    "target_layer_id=13\n",
    "\n",
    "hiddens_list=[]\n",
    "hiddens_tv_list=[]\n",
    "for lg_id in [0,1,2]:\n",
    "    prompts=task.few_shot[lg_id]\n",
    "    outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "    hiddens=extract_hidden(outputs, layer_id=source_layer_id)\n",
    "    hiddens_list.append(hiddens)\n",
    "    hiddens_tv=extract_hidden(outputs, layer_id=target_layer_id)\n",
    "    hiddens_tv_list.append(hiddens_tv)\n",
    "\n",
    "DATA=torch.cat(hiddens_list,dim=0)\n",
    "data=DATA.cpu().numpy()\n",
    "\n",
    "DATA_tv=torch.cat(hiddens_tv_list,dim=0)\n",
    "data_tv=DATA_tv.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "tsne_result = tsne.fit_transform(data)\n",
    "\n",
    "# Plotting the explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "label_list=[\"fr2en\",\"sp2en\",\"it2en\"]\n",
    "class_size=DATA.size(1)\n",
    "for index in range(3):\n",
    "    plt.title('PCA')\n",
    "    pca_result_class = pca_result[batch_size*index:batch_size*(index+1),:]\n",
    "    plt.scatter(pca_result_class[:, 0], pca_result_class[:, 1], label=label_list[index],alpha=0.7)\n",
    "    # plt.title('tSNE')\n",
    "    # tsne_result_class = tsne_result[batch_size*index:batch_size*(index+1),:]\n",
    "    # plt.scatter(tsne_result_class[:, 0], tsne_result_class[:, 1], label=label_list[index],alpha=0.7)\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
