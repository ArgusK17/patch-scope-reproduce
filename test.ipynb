{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "import transformers\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Change it to your own path.\n",
    "path=__path__\n",
    "\n",
    "# I use Llama-2, but I believe that any LLM supported by the Transformers can be easily adapted.\n",
    "tokenizer = LlamaTokenizer.from_pretrained(path, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'Amazon', \"'\", 's', 'former', 'CE', 'O', 'attended', 'O', 'sc', 'ars']]\n",
      "['hello;']\n"
     ]
    }
   ],
   "source": [
    "from tools.utils import extract_hidden, break2tokens\n",
    "from tools.patch_scope import patch_scope\n",
    "from tools.inference import forward\n",
    "\n",
    "# All index start from 0 (token position and layer index)\n",
    "\n",
    "prompts=[\"Amazon's former CEO attended Oscars\"]\n",
    "print(break2tokens(tokenizer,prompts))\n",
    "outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "# From token position 6 extract the hidden state after layer 7\n",
    "hiddens=extract_hidden(outputs, token_pos=6, layer_id=7)\n",
    "\n",
    "# Patch the hidden to target position\n",
    "result=patch_scope(model, tokenizer, hiddens, \n",
    "    target_prompt = 'cat->cat; 135->135; hello->hello; ?->', \n",
    "    target_token_pos = -2,\n",
    "    target_layer_id = 7,\n",
    "    gen_len = 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'le', 'mon', '->', 'yellow', ',', 'gra', 'pe', '->', 'pur', 'ple', ',', 'apple', '->']]\n",
      "['apple']\n"
     ]
    }
   ],
   "source": [
    "prompts=[\"lemon->yellow, grape->purple, apple->\"]\n",
    "print(break2tokens(tokenizer,prompts))\n",
    "outputs=forward(model, tokenizer, prompts, with_hiddens=True)\n",
    "hiddens=extract_hidden(outputs, token_pos=12, layer_id=7)\n",
    "\n",
    "result=patch_scope(model, tokenizer, hiddens, \n",
    "    target_prompt = 'cat->cat; 135->135; hello->hello; ?->', \n",
    "    target_token_pos = -2,\n",
    "    target_layer_id = 7,\n",
    "    gen_len = 1)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama)",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
